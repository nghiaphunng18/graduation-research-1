# Tá»•ng quan vá» Transformer

Transformer lÃ  má»™t kiáº¿n trÃºc máº¡ng nÆ¡-ron nhÃ¢n táº¡o mang tÃ­nh Ä‘á»™t phÃ¡, Ä‘Æ°á»£c giá»›i thiá»‡u vÃ o nÄƒm 2017 trong bÃ i bÃ¡o *"Attention is All You Need"*. Ban Ä‘áº§u Ä‘Æ°á»£c thiáº¿t káº¿ cho cÃ¡c bÃ i toÃ¡n xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP) nhÆ° dá»‹ch mÃ¡y, Transformer Ä‘Ã£ trá»Ÿ thÃ nh ná»n táº£ng cho cÃ¡c mÃ´ hÃ¬nh tiÃªn tiáº¿n nhÆ° BERT, GPT, T5, vÃ  nhiá»u á»©ng dá»¥ng AI khÃ¡c. KhÃ¡c vá»›i cÃ¡c mÃ´ hÃ¬nh truyá»n thá»‘ng nhÆ° RNN hay LSTM, Transformer sá»­ dá»¥ng cÆ¡ cháº¿ **Attention** Ä‘á»ƒ xá»­ lÃ½ chuá»—i dá»¯ liá»‡u má»™t cÃ¡ch song song, cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u suáº¥t vÃ  kháº£ nÄƒng náº¯m báº¯t má»‘i quan há»‡ phá»©c táº¡p trong vÄƒn báº£n. BÃ¡o cÃ¡o nÃ y sáº½ trÃ¬nh bÃ y chi tiáº¿t cÃ¡ch Transformer hoáº¡t Ä‘á»™ng, cÃ¡c thÃ nh pháº§n cá»‘t lÃµi, quy trÃ¬nh huáº¥n luyá»‡n, á»©ng dá»¥ng thá»±c tiá»…n, vÃ  nhá»¯ng thÃ¡ch thá»©c liÃªn quan.

## 1. Transformer lÃ  gÃ¬?

Transformer lÃ  má»™t mÃ´ hÃ¬nh sequence-to-sequence, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ chuyá»ƒn Ä‘á»•i má»™t chuá»—i Ä‘áº§u vÃ o (vÃ­ dá»¥: má»™t cÃ¢u tiáº¿ng Viá»‡t) thÃ nh má»™t chuá»—i Ä‘áº§u ra (vÃ­ dá»¥: cÃ¢u tiáº¿ng Anh). Äiá»ƒm Ä‘áº·c biá»‡t cá»§a Transformer náº±m á»Ÿ viá»‡c sá»­ dá»¥ng cÆ¡ cháº¿ **Self-Attention**, cho phÃ©p mÃ´ hÃ¬nh xá»­ lÃ½ toÃ n bá»™ chuá»—i cÃ¹ng lÃºc thay vÃ¬ tuáº§n tá»±, giÃºp:
- TÄƒng tá»‘c Ä‘á»™ huáº¥n luyá»‡n nhá» tÃ­nh toÃ¡n song song.
- Náº¯m báº¯t má»‘i quan há»‡ giá»¯a cÃ¡c tá»« trong chuá»—i, báº¥t ká»ƒ khoáº£ng cÃ¡ch giá»¯a chÃºng.
- Há»— trá»£ cÃ¡c bÃ i toÃ¡n phá»©c táº¡p trong NLP vÃ  hÆ¡n tháº¿ ná»¯a (nhÆ° xá»­ lÃ½ hÃ¬nh áº£nh vá»›i Vision Transformer).

Transformer Ä‘Ã£ Ä‘Æ°á»£c á»©ng dá»¥ng rá»™ng rÃ£i trong cÃ¡c lÄ©nh vá»±c:
- **Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP)**: Dá»‹ch mÃ¡y, tÃ³m táº¯t vÄƒn báº£n, táº¡o vÄƒn báº£n, nháº­n dáº¡ng thá»±c thá»ƒ (NER), há»i Ä‘Ã¡p.
- **TrÃ­ tuá»‡ nhÃ¢n táº¡o tá»•ng quÃ¡t**: CÃ¡c mÃ´ hÃ¬nh nhÆ° GPT-3 sá»­ dá»¥ng Transformer Ä‘á»ƒ thá»±c hiá»‡n cÃ¡c tÃ¡c vá»¥ Ä‘a dáº¡ng mÃ  khÃ´ng cáº§n tinh chá»‰nh.
- **á»¨ng dá»¥ng ngoÃ i NLP**: Xá»­ lÃ½ hÃ¬nh áº£nh (Vision Transformer), Ã¢m thanh (WaveNet), vÃ  trÃ² chÆ¡i (AlphaStar).

VÃ­ dá»¥, mÃ´ hÃ¬nh GPT-2 cá»§a OpenAI sá»­ dá»¥ng kiáº¿n trÃºc Transformer Ä‘á»ƒ táº¡o vÄƒn báº£n giá»‘ng con ngÆ°á»i, trong khi AlphaStar cá»§a DeepMind táº­n dá»¥ng Transformer Ä‘á»ƒ phÃ¢n tÃ­ch chiáº¿n lÆ°á»£c trong trÃ² chÆ¡i StarCraft.

## 2. So sÃ¡nh Transformer vá»›i RNN vÃ  LSTM

### 2.1. Recurrent Neural Network (RNN)
![alt text](image-1.png)
RNN Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u chuá»—i báº±ng cÃ¡ch duy trÃ¬ má»™t **tráº¡ng thÃ¡i áº©n** (hidden state) truyá»n thÃ´ng tin tá»« bÆ°á»›c thá»i gian trÆ°á»›c sang bÆ°á»›c tiáº¿p theo. CÃ¡ch hoáº¡t Ä‘á»™ng:
- **Kiáº¿n trÃºc**: RNN chá»©a má»™t vÃ²ng láº·p, nÆ¡i Ä‘áº§u ra cá»§a bÆ°á»›c trÆ°á»›c trá»Ÿ thÃ nh Ä‘áº§u vÃ o cá»§a bÆ°á»›c tiáº¿p theo. VÃ­ dá»¥, khi xá»­ lÃ½ cÃ¢u "TÃ´i thÃ­ch há»c", RNN sáº½ xá»­ lÃ½ tá»«ng tá»« ("TÃ´i", "thÃ­ch", "há»c") vÃ  truyá»n tráº¡ng thÃ¡i áº©n qua má»—i bÆ°á»›c.
- **Æ¯u Ä‘iá»ƒm**:
  - PhÃ¹ há»£p vá»›i dá»¯ liá»‡u chuá»—i cÃ³ Ä‘á»™ dÃ i thay Ä‘á»•i.
  - CÃ³ thá»ƒ lÆ°u giá»¯ thÃ´ng tin ngá»¯ cáº£nh qua cÃ¡c bÆ°á»›c.
- **Háº¡n cháº¿**:
  - **Xá»­ lÃ½ tuáº§n tá»±**: RNN pháº£i xá»­ lÃ½ tá»«ng tá»« má»™t, khÃ´ng thá»ƒ táº­n dá»¥ng tÃ­nh toÃ¡n song song, dáº«n Ä‘áº¿n thá»i gian huáº¥n luyá»‡n lÃ¢u trÃªn chuá»—i dÃ i.
  - **Vanishing Gradient**: Khi chuá»—i dÃ i, gradient cÃ³ thá»ƒ trá»Ÿ nÃªn quÃ¡ nhá», khiáº¿n mÃ´ hÃ¬nh khÃ³ há»c Ä‘Æ°á»£c cÃ¡c má»‘i quan há»‡ giá»¯a cÃ¡c tá»« cÃ¡ch xa nhau. VÃ­ dá»¥, trong cÃ¢u "Há»“ ChÃ­ Minh, ngÆ°á»i anh hÃ¹ng dÃ¢n tá»™c, Ä‘Ã£ lÃ£nh Ä‘áº¡o...", RNN cÃ³ thá»ƒ khÃ³ liÃªn káº¿t "Há»“ ChÃ­ Minh" vá»›i "lÃ£nh Ä‘áº¡o" do khoáº£ng cÃ¡ch lá»›n.
  - **KhÃ³ khÄƒn vá»›i ngá»¯ cáº£nh xa**: RNN thÆ°á»ng Æ°u tiÃªn thÃ´ng tin gáº§n hÆ¡n, lÃ m máº¥t thÃ´ng tin tá»« cÃ¡c tá»« Ä‘áº§u chuá»—i.

### 2.2. Long Short-Term Memory (LSTM)
![alt text](image-2.png)
LSTM lÃ  má»™t biáº¿n thá»ƒ cáº£i tiáº¿n cá»§a RNN, sá»­ dá»¥ng **Ã´ nhá»›** (memory cell) vÃ  cÃ¡c **cá»•ng** (gates: forget, input, output) Ä‘á»ƒ kiá»ƒm soÃ¡t luá»“ng thÃ´ng tin. CÃ¡ch hoáº¡t Ä‘á»™ng:
- **Kiáº¿n trÃºc**: LSTM quyáº¿t Ä‘á»‹nh thÃ´ng tin nÃ o cáº§n giá»¯ láº¡i hoáº·c quÃªn Ä‘i qua cÃ¡c cá»•ng. VÃ­ dá»¥, trong cÃ¢u trÃªn, LSTM cÃ³ thá»ƒ chá»n giá»¯ thÃ´ng tin vá» "Há»“ ChÃ­ Minh" Ä‘á»ƒ sá»­ dá»¥ng khi xá»­ lÃ½ "lÃ£nh Ä‘áº¡o".
- **Æ¯u Ä‘iá»ƒm**:
  - Giáº£i quyáº¿t váº¥n Ä‘á» vanishing gradient báº±ng cÃ¡ch duy trÃ¬ thÃ´ng tin quan trá»ng qua chuá»—i dÃ i.
  - CÃ³ kháº£ nÄƒng há»c cÃ¡c má»‘i quan há»‡ phá»©c táº¡p hÆ¡n so vá»›i RNN.
- **Háº¡n cháº¿**:
  - **Váº«n tuáº§n tá»±**: LSTM váº«n xá»­ lÃ½ chuá»—i theo thá»© tá»±, háº¡n cháº¿ kháº£ nÄƒng song song hÃ³a.
  - **Hiá»‡u suáº¥t giáº£m vá»›i chuá»—i ráº¥t dÃ i**: Máº·c dÃ¹ cáº£i tiáº¿n hÆ¡n RNN, LSTM váº«n gáº·p khÃ³ khÄƒn khi chuá»—i vÆ°á»£t quÃ¡ hÃ ng trÄƒm tá»«, Ä‘áº·c biá»‡t vá»›i cÃ¡c má»‘i quan há»‡ ngá»¯ cáº£nh phá»©c táº¡p.
  - **Chi phÃ­ tÃ­nh toÃ¡n cao**: LSTM phá»©c táº¡p hÆ¡n RNN, Ä‘Ã²i há»i nhiá»u tÃ i nguyÃªn hÆ¡n.

### 2.3. Transformer vÆ°á»£t trá»™i nhÆ° tháº¿ nÃ o?
Transformer kháº¯c phá»¥c cÃ¡c háº¡n cháº¿ cá»§a RNN vÃ  LSTM thÃ´ng qua:
- **TÃ­nh toÃ¡n song song**: Thay vÃ¬ xá»­ lÃ½ tuáº§n tá»±, Transformer sá»­ dá»¥ng Self-Attention Ä‘á»ƒ xá»­ lÃ½ toÃ n bá»™ chuá»—i cÃ¹ng lÃºc, táº­n dá»¥ng tá»‘i Ä‘a pháº§n cá»©ng nhÆ° GPU.
- **Náº¯m báº¯t má»‘i quan há»‡ xa**: Self-Attention cho phÃ©p mÃ´ hÃ¬nh liÃªn káº¿t cÃ¡c tá»« cÃ¡ch xa nhau mÃ  khÃ´ng cáº§n truyá»n thÃ´ng tin qua nhiá»u bÆ°á»›c. VÃ­ dá»¥, trong cÃ¢u "Há»“ ChÃ­ Minh, ngÆ°á»i anh hÃ¹ng dÃ¢n tá»™c, Ä‘Ã£ lÃ£nh Ä‘áº¡o...", Transformer dá»… dÃ ng liÃªn káº¿t "Há»“ ChÃ­ Minh" vá»›i "lÃ£nh Ä‘áº¡o".
- **Hiá»‡u quáº£ cao**: Nhá» song song hÃ³a vÃ  kiáº¿n trÃºc tá»‘i Æ°u, Transformer huáº¥n luyá»‡n nhanh hÆ¡n vÃ  xá»­ lÃ½ Ä‘Æ°á»£c cÃ¡c táº­p dá»¯ liá»‡u lá»›n.
- **Linh hoáº¡t**: Transformer khÃ´ng chá»‰ giá»›i háº¡n á»Ÿ NLP mÃ  cÃ²n Ä‘Æ°á»£c Ã¡p dá»¥ng trong cÃ¡c lÄ©nh vá»±c nhÆ° thá»‹ giÃ¡c mÃ¡y tÃ­nh (Vision Transformer) vÃ  xá»­ lÃ½ Ã¢m thanh.

## 3. Kiáº¿n trÃºc Transformer

Transformer bao gá»“m hai thÃ nh pháº§n chÃ­nh: **Encoder** (bá»™ mÃ£ hÃ³a) vÃ  **Decoder** (bá»™ giáº£i mÃ£). Má»—i thÃ nh pháº§n lÃ  má»™t ngÄƒn xáº¿p (stack) gá»“m nhiá»u táº§ng (layer) giá»‘ng nhau (thÆ°á»ng lÃ  6 táº§ng má»—i bÃªn trong mÃ´ hÃ¬nh gá»‘c, nhÆ°ng cÃ³ thá»ƒ thay Ä‘á»•i tÃ¹y mÃ´ hÃ¬nh).

### 3.1. Tá»•ng quan kiáº¿n trÃºc
- **Encoder**: Nháº­n chuá»—i Ä‘áº§u vÃ o (vÃ­ dá»¥: cÃ¢u tiáº¿ng Viá»‡t) vÃ  mÃ£ hÃ³a nÃ³ thÃ nh má»™t táº­p há»£p cÃ¡c biá»ƒu diá»…n (representation) chá»©a thÃ´ng tin ngá»¯ cáº£nh. Encoder phÃ¹ há»£p cho cÃ¡c bÃ i toÃ¡n yÃªu cáº§u hiá»ƒu vÄƒn báº£n, nhÆ° phÃ¢n loáº¡i cáº£m xÃºc hoáº·c nháº­n dáº¡ng thá»±c thá»ƒ.
- **Decoder**: Sá»­ dá»¥ng cÃ¡c biá»ƒu diá»…n tá»« Encoder vÃ  chuá»—i Ä‘áº§u ra hiá»‡n táº¡i Ä‘á»ƒ sinh ra chuá»—i Ä‘áº§u ra hoÃ n chá»‰nh (vÃ­ dá»¥: cÃ¢u tiáº¿ng Anh). Decoder phÃ¹ há»£p cho cÃ¡c bÃ i toÃ¡n táº¡o sinh, nhÆ° dá»‹ch mÃ¡y hoáº·c táº¡o vÄƒn báº£n.
- **Káº¿t ná»‘i**: Encoder vÃ  Decoder Ä‘Æ°á»£c káº¿t ná»‘i thÃ´ng qua má»™t lá»›p **Encoder-Decoder Attention**, cho phÃ©p Decoder táº­p trung vÃ o cÃ¡c pháº§n quan trá»ng cá»§a chuá»—i Ä‘áº§u vÃ o.

HÃ¬nh minh há»a kiáº¿n trÃºc Transformer:

![alt text](image.png)

### 3.2. Encoder
![alt text](image-3.png)
Má»—i táº§ng Encoder bao gá»“m hai lá»›p con chÃ­nh:
1. **Self-Attention**:
   - Cho phÃ©p má»—i tá»« trong chuá»—i Ä‘áº§u vÃ o "chÃº Ã½" Ä‘áº¿n táº¥t cáº£ cÃ¡c tá»« khÃ¡c, giÃºp náº¯m báº¯t má»‘i quan há»‡ ngá»¯ cáº£nh. VÃ­ dá»¥, trong cÃ¢u "Há»“ ChÃ­ Minh lÃ  vá»‹ cha giÃ  kÃ­nh yÃªu", tá»« "NgÆ°á»i" cáº§n chÃº Ã½ Ä‘áº¿n "Há»“ ChÃ­ Minh" Ä‘á»ƒ hiá»ƒu má»‘i quan há»‡ Ä‘á»“ng nháº¥t.
   - Self-Attention táº¡o ra má»™t biá»ƒu diá»…n má»›i cho má»—i tá»«, káº¿t há»£p thÃ´ng tin tá»« cÃ¡c tá»« khÃ¡c dá»±a trÃªn má»©c Ä‘á»™ liÃªn quan.
2. **Feed-Forward Neural Network (FNN)**:
   - Ãp dá»¥ng má»™t máº¡ng nÆ¡-ron truyá»n tháº³ng Ä‘á»™c láº­p cho tá»«ng tá»«, tÄƒng kháº£ nÄƒng biá»ƒu diá»…n. Máº¡ng nÃ y gá»“m hai táº§ng tuyáº¿n tÃ­nh vá»›i hÃ m kÃ­ch hoáº¡t ReLU á»Ÿ giá»¯a: `FFN(x) = max(0, xW1 + b1)W2 + b2`.
   - Máº·c dÃ¹ Ã¡p dá»¥ng Ä‘á»™c láº­p, cÃ¡c tá»« chia sáº» cÃ¹ng má»™t bá»™ trá»ng sá»‘, giÃºp giáº£m sá»‘ lÆ°á»£ng tham sá»‘.

**CÃ¡c thÃ nh pháº§n bá»• sung**:
- **Káº¿t ná»‘i Residual**: Má»—i lá»›p con (Self-Attention vÃ  FNN) Ä‘Æ°á»£c bao quanh bá»Ÿi má»™t káº¿t ná»‘i residual, nÆ¡i Ä‘áº§u vÃ o Ä‘Æ°á»£c cá»™ng trá»±c tiáº¿p vÃ o Ä‘áº§u ra: `Output = LayerNorm(x + Sublayer(x))`. Äiá»u nÃ y giÃºp giáº£m váº¥n Ä‘á» vanishing gradient vÃ  cáº£i thiá»‡n luá»“ng thÃ´ng tin.
- **Chuáº©n hÃ³a táº§ng (Layer Normalization)**: á»”n Ä‘á»‹nh quÃ¡ trÃ¬nh huáº¥n luyá»‡n báº±ng cÃ¡ch chuáº©n hÃ³a Ä‘áº§u ra cá»§a má»—i lá»›p con theo trung bÃ¬nh vÃ  phÆ°Æ¡ng sai.

### 3.3. Decoder
![alt text](image-4.png)
Má»—i táº§ng Decoder cÃ³ ba lá»›p con:
1. **Masked Self-Attention**:
   - TÆ°Æ¡ng tá»± Self-Attention, nhÆ°ng chá»‰ cho phÃ©p chÃº Ã½ Ä‘áº¿n cÃ¡c tá»« trÆ°á»›c Ä‘Ã³ trong chuá»—i Ä‘áº§u ra (Ä‘á»ƒ trÃ¡nh "nhÃ¬n trÆ°á»›c" káº¿t quáº£). VÃ­ dá»¥, khi sinh tá»« thá»© 4 trong cÃ¢u Ä‘áº§u ra, Decoder chá»‰ Ä‘Æ°á»£c sá»­ dá»¥ng thÃ´ng tin tá»« 3 tá»« Ä‘áº§u tiÃªn.
   - Masking Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng cÃ¡ch Ä‘áº·t cÃ¡c Ä‘iá»ƒm Attention cá»§a cÃ¡c tá»« tÆ°Æ¡ng lai thÃ nh -âˆ trÆ°á»›c khi Ã¡p dá»¥ng hÃ m Softmax.
2. **Encoder-Decoder Attention**:
   - Cho phÃ©p Decoder táº­p trung vÃ o cÃ¡c pháº§n quan trá»ng cá»§a chuá»—i Ä‘áº§u vÃ o tá»« Encoder. VÃ­ dá»¥, khi dá»‹ch "Je suis Ã©tudiant" sang "I am a student", Decoder sáº½ chÃº Ã½ Ä‘áº¿n "Ã©tudiant" khi sinh tá»« "student".
   - Lá»›p nÃ y sá»­ dá»¥ng cÃ¡c vector Key vÃ  Value tá»« Encoder, trong khi Query Ä‘áº¿n tá»« Decoder.
3. **Feed-Forward Neural Network**:
   - TÆ°Æ¡ng tá»± nhÆ° trong Encoder, Ã¡p dá»¥ng máº¡ng truyá»n tháº³ng Ä‘á»™c láº­p cho má»—i tá»«.

Decoder cÅ©ng sá»­ dá»¥ng káº¿t ná»‘i Residual vÃ  chuáº©n hÃ³a táº§ng Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t.

### 3.4. Positional Encoding
VÃ¬ Transformer khÃ´ng xá»­ lÃ½ chuá»—i theo thá»© tá»± tuáº§n tá»± nhÆ° RNN, nÃ³ cáº§n má»™t cÃ¡ch Ä‘á»ƒ mÃ£ hÃ³a vá»‹ trÃ­ cá»§a cÃ¡c tá»«. **Positional Encoding** sá»­ dá»¥ng cÃ¡c hÃ m sin vÃ  cos Ä‘á»ƒ táº¡o ra cÃ¡c vector vá»‹ trÃ­, Ä‘Æ°á»£c cá»™ng vÃ o vector embedding cá»§a tá»«. Äiá»u nÃ y giÃºp mÃ´ hÃ¬nh hiá»ƒu Ä‘Æ°á»£c thá»© tá»± tá»« trong chuá»—i.

**CÃ´ng thá»©c Positional Encoding**:
```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```
Trong Ä‘Ã³:
- `pos`: Vá»‹ trÃ­ cá»§a tá»« trong chuá»—i (báº¯t Ä‘áº§u tá»« 0).
- `i`: Chá»‰ sá»‘ chiá»u cá»§a vector (tá»« 0 Ä‘áº¿n `d_model/2`).
- `d_model`: KÃ­ch thÆ°á»›c cá»§a vector embedding (thÆ°á»ng lÃ  512).

**LÃ½ do sá»­ dá»¥ng sin vÃ  cos**:
- CÃ¡c hÃ m nÃ y táº¡o ra cÃ¡c giÃ¡ trá»‹ tuáº§n hoÃ n, giÃºp mÃ´ hÃ¬nh suy ra vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i giá»¯a cÃ¡c tá»«.
- MÃ´ hÃ¬nh cÃ³ thá»ƒ tá»•ng quÃ¡t hÃ³a cho cÃ¡c chuá»—i dÃ i hÆ¡n táº­p huáº¥n luyá»‡n nhá» tÃ­nh cháº¥t tuyáº¿n tÃ­nh cá»§a sin vÃ  cos.

VÃ­ dá»¥, vá»›i `d_model = 4`, Positional Encoding cho tá»« á»Ÿ vá»‹ trÃ­ 0 cÃ³ thá»ƒ lÃ :
```
[sin(0/10000^(0/4)), cos(0/10000^(0/4)), sin(0/10000^(2/4)), cos(0/10000^(2/4))] = [0, 1, 0, 1]
```

### 3.5. Self-Attention vÃ  Multi-Head Attention
CÆ¡ cháº¿ **Self-Attention** lÃ  trÃ¡i tim cá»§a Transformer, cho phÃ©p mÃ´ hÃ¬nh cÃ¢n nháº¯c má»©c Ä‘á»™ quan trá»ng cá»§a cÃ¡c tá»« khÃ¡c nhau trong chuá»—i. Quy trÃ¬nh tÃ­nh Self-Attention bao gá»“m cÃ¡c bÆ°á»›c sau:

1. **Táº¡o vector Query, Key, Value**:
   - Má»—i tá»« Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng má»™t vector embedding (kÃ­ch thÆ°á»›c `d_model`).
   - NhÃ¢n embedding vá»›i ba ma tráº­n trá»ng sá»‘ (`W_Q`, `W_K`, `W_V`) Ä‘á»ƒ táº¡o ra vector **Query (Q)**, **Key (K)**, vÃ  **Value (V)**, má»—i vector cÃ³ kÃ­ch thÆ°á»›c `d_k` (thÆ°á»ng nhá» hÆ¡n `d_model`, vÃ­ dá»¥ 64).
   - CÃ´ng thá»©c: `Q = X * W_Q`, `K = X * W_K`, `V = X * W_V`, trong Ä‘Ã³ `X` lÃ  ma tráº­n embedding cá»§a chuá»—i.

2. **TÃ­nh Ä‘iá»ƒm Attention**:
   - TÃ­nh tÃ­ch vÃ´ hÆ°á»›ng giá»¯a Query cá»§a tá»« hiá»‡n táº¡i vÃ  Key cá»§a táº¥t cáº£ cÃ¡c tá»«: `score = Q * K^T`.
   - Chuáº©n hÃ³a Ä‘iá»ƒm báº±ng cÃ¡ch chia cho cÄƒn báº­c hai cá»§a chiá»u Key (`sqrt(d_k)`) Ä‘á»ƒ á»•n Ä‘á»‹nh Ä‘á»™ dá»‘c: `score = (Q * K^T) / sqrt(d_k)`.
   - Ãp dá»¥ng hÃ m **Softmax** Ä‘á»ƒ chuyá»ƒn Ä‘iá»ƒm thÃ nh xÃ¡c suáº¥t: `attention_weights = softmax(score)`.

3. **TÃ­nh Ä‘áº§u ra**:
   - NhÃ¢n xÃ¡c suáº¥t vá»›i vector Value vÃ  cá»™ng láº¡i Ä‘á»ƒ táº¡o ra vector Ä‘áº§u ra cho tá»« hiá»‡n táº¡i: `output = attention_weights * V`.

**CÃ´ng thá»©c tá»•ng quÃ¡t**:
```
Attention(Q, K, V) = softmax((Q * K^T) / sqrt(d_k)) * V
```

**Multi-Head Attention** cáº£i tiáº¿n Self-Attention báº±ng cÃ¡ch:
- Chia vector Q, K, V thÃ nh `h` "Ä‘áº§u" (head) Ä‘á»™c láº­p (thÆ°á»ng `h = 8`).
- TÃ­nh Self-Attention riÃªng cho tá»«ng Ä‘áº§u, má»—i Ä‘áº§u cÃ³ kÃ­ch thÆ°á»›c `d_k = d_model / h`.
- Ná»‘i káº¿t quáº£ tá»« cÃ¡c Ä‘áº§u vÃ  Ã¡p dá»¥ng má»™t ma tráº­n trá»ng sá»‘ (`W_O`) Ä‘á»ƒ táº¡o Ä‘áº§u ra cuá»‘i cÃ¹ng: `MultiHead(Q, K, V) = Concat(head_1, ..., head_h) * W_O`.
- **Lá»£i Ã­ch**:
  - Cho phÃ©p mÃ´ hÃ¬nh náº¯m báº¯t nhiá»u khÃ­a cáº¡nh khÃ¡c nhau cá»§a má»‘i quan há»‡ giá»¯a cÃ¡c tá»« (vÃ­ dá»¥: ngá»¯ phÃ¡p, ngá»¯ nghÄ©a, quan há»‡ tá»« vá»±ng).
  - TÄƒng tÃ­nh biá»ƒu diá»…n mÃ  khÃ´ng lÃ m tÄƒng Ä‘Ã¡ng ká»ƒ chi phÃ­ tÃ­nh toÃ¡n.

HÃ¬nh minh há»a Self-Attention:

![alt text](image-6.png)

### 3.6. Luá»“ng thÃ´ng tin trong Transformer
QuÃ¡ trÃ¬nh xá»­ lÃ½ má»™t chuá»—i trong Transformer diá»…n ra nhÆ° sau:
1. **Äáº§u vÃ o**:
   - Chuá»—i Ä‘áº§u vÃ o (vÃ­ dá»¥: "TÃ´i thÃ­ch há»c") Ä‘Æ°á»£c chuyá»ƒn thÃ nh cÃ¡c vector embedding thÃ´ng qua má»™t báº£ng tra cá»©u tá»« vá»±ng (word embedding).
   - ThÃªm Positional Encoding vÃ o embedding Ä‘á»ƒ mÃ£ hÃ³a vá»‹ trÃ­ tá»«.
2. **Encoder**:
   - Chuá»—i embedding Ä‘i qua cÃ¡c táº§ng Encoder, má»—i táº§ng Ã¡p dá»¥ng Self-Attention vÃ  FNN.
   - Äáº§u ra cá»§a Encoder lÃ  má»™t táº­p há»£p cÃ¡c vector biá»ƒu diá»…n ngá»¯ cáº£nh phong phÃº.
3. **Decoder**:
   - Chuá»—i Ä‘áº§u ra (ban Ä‘áº§u cÃ³ thá»ƒ lÃ  má»™t token Ä‘áº·c biá»‡t `<start>`) Ä‘Æ°á»£c chuyá»ƒn thÃ nh embedding vÃ  thÃªm Positional Encoding.
   - Decoder sá»­ dá»¥ng Masked Self-Attention Ä‘á»ƒ xá»­ lÃ½ chuá»—i Ä‘áº§u ra hiá»‡n táº¡i, sau Ä‘Ã³ dÃ¹ng Encoder-Decoder Attention Ä‘á»ƒ káº¿t há»£p thÃ´ng tin tá»« Encoder.
   - Äáº§u ra cuá»‘i cÃ¹ng Ä‘i qua má»™t táº§ng tuyáº¿n tÃ­nh vÃ  hÃ m Softmax Ä‘á»ƒ dá»± Ä‘oÃ¡n tá»« tiáº¿p theo.
4. **Láº·p láº¡i**:
   - Decoder sinh tá»«ng tá»« má»™t, thÃªm tá»« má»›i vÃ o chuá»—i Ä‘áº§u ra vÃ  tiáº¿p tá»¥c cho Ä‘áº¿n khi gáº·p token Ä‘áº·c biá»‡t `<end>`.

## 4. Quy trÃ¬nh huáº¥n luyá»‡n Transformer

Huáº¥n luyá»‡n Transformer lÃ  má»™t quÃ¡ trÃ¬nh phá»©c táº¡p, bao gá»“m hai giai Ä‘oáº¡n chÃ­nh: **huáº¥n luyá»‡n trÆ°á»›c (pre-training)** vÃ  **tinh chá»‰nh (fine-tuning)**.

### 4.1. Huáº¥n luyá»‡n trÆ°á»›c (Pre-training)
- **Má»¥c tiÃªu**: Há»c biá»ƒu diá»…n ngÃ´n ngá»¯ chung tá»« má»™t lÆ°á»£ng lá»›n dá»¯ liá»‡u thÃ´ (vÃ­ dá»¥: vÄƒn báº£n tá»« internet, sÃ¡ch, bÃ i bÃ¡o).
- **PhÆ°Æ¡ng phÃ¡p**:
  - **Há»c tá»± giÃ¡m sÃ¡t (Self-Supervised Learning)**: MÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n mÃ  khÃ´ng cáº§n nhÃ£n do con ngÆ°á»i cung cáº¥p. Thay vÃ o Ä‘Ã³, má»¥c tiÃªu Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng tá»« dá»¯ liá»‡u.
  - **MÃ´ hÃ¬nh ngÃ´n ngá»¯ nhÃ¢n quáº£ (Causal Language Modeling)**: Dá»± Ä‘oÃ¡n tá»« tiáº¿p theo trong chuá»—i dá»±a trÃªn cÃ¡c tá»« trÆ°á»›c Ä‘Ã³. VÃ­ dá»¥, vá»›i cÃ¢u "TÃ´i thÃ­ch há»c", mÃ´ hÃ¬nh há»c dá»± Ä‘oÃ¡n "há»c" tá»« "TÃ´i thÃ­ch".
  - **MÃ´ hÃ¬nh ngÃ´n ngá»¯ che dáº¥u (Masked Language Modeling)**: Che ngáº«u nhiÃªn má»™t sá»‘ tá»« trong cÃ¢u vÃ  yÃªu cáº§u mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n chÃºng. VÃ­ dá»¥, vá»›i cÃ¢u "TÃ´i [MASK] há»c", mÃ´ hÃ¬nh pháº£i Ä‘oÃ¡n tá»« bá»‹ che lÃ  "thÃ­ch".
- **Dá»¯ liá»‡u**: CÃ¡c táº­p dá»¯ liá»‡u lá»›n nhÆ° Wikipedia, Common Crawl, hoáº·c BookCorpus.
- **Chi phÃ­**: Huáº¥n luyá»‡n trÆ°á»›c Ä‘Ã²i há»i tÃ i nguyÃªn tÃ­nh toÃ¡n khá»•ng lá»“, thÆ°á»ng kÃ©o dÃ i vÃ i tuáº§n trÃªn hÃ ng trÄƒm GPU.

### 4.2. Tinh chá»‰nh (Fine-tuning)
- **Má»¥c tiÃªu**: Äiá»u chá»‰nh mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n trÆ°á»›c cho má»™t bÃ i toÃ¡n cá»¥ thá»ƒ (vÃ­ dá»¥: dá»‹ch mÃ¡y, phÃ¢n loáº¡i cáº£m xÃºc).
- **PhÆ°Æ¡ng phÃ¡p**:
  - **Há»c cÃ³ giÃ¡m sÃ¡t (Supervised Learning)**: Sá»­ dá»¥ng dá»¯ liá»‡u cÃ³ nhÃ£n (vÃ­ dá»¥: cáº·p cÃ¢u tiáº¿ng Viá»‡t - tiáº¿ng Anh cho dá»‹ch mÃ¡y).
  - Huáº¥n luyá»‡n mÃ´ hÃ¬nh trÃªn táº­p dá»¯ liá»‡u nhá» hÆ¡n, vá»›i cÃ¡c tham sá»‘ Ä‘Æ°á»£c khá»Ÿi táº¡o tá»« mÃ´ hÃ¬nh huáº¥n luyá»‡n trÆ°á»›c.
- **Lá»£i Ã­ch**:
  - Táº­n dá»¥ng kiáº¿n thá»©c ngÃ´n ngá»¯ chung tá»« huáº¥n luyá»‡n trÆ°á»›c, giáº£m nhu cáº§u dá»¯ liá»‡u cÃ³ nhÃ£n.
  - Thá»i gian huáº¥n luyá»‡n ngáº¯n hÆ¡n vÃ  chi phÃ­ tháº¥p hÆ¡n so vá»›i huáº¥n luyá»‡n tá»« Ä‘áº§u.
- **VÃ­ dá»¥**: Tinh chá»‰nh BERT Ä‘á»ƒ phÃ¢n loáº¡i cáº£m xÃºc báº±ng cÃ¡ch thÃªm má»™t táº§ng phÃ¢n loáº¡i vÃ o Ä‘áº§u ra cá»§a Encoder vÃ  huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u cÃ³ nhÃ£n nhÆ° "TÃ­ch cá»±c" hoáº·c "TiÃªu cá»±c".

### 4.3. HÃ m máº¥t mÃ¡t (Loss Function)
- **Cross-Entropy Loss**: ÄÆ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘o lÆ°á»ng sá»± khÃ¡c biá»‡t giá»¯a phÃ¢n phá»‘i xÃ¡c suáº¥t dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh vÃ  nhÃ£n Ä‘Ãºng. VÃ­ dá»¥, khi dá»‹ch "merci" sang "thanks", mÃ´ hÃ¬nh cáº§n tá»‘i Æ°u Ä‘á»ƒ xÃ¡c suáº¥t cá»§a "thanks" lÃ  cao nháº¥t.
- **Kullback-Leibler Divergence**: ÄÃ´i khi Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ so sÃ¡nh phÃ¢n phá»‘i xÃ¡c suáº¥t, Ä‘áº·c biá»‡t trong cÃ¡c bÃ i toÃ¡n tinh chá»‰nh.

### 4.4. Tá»‘i Æ°u hÃ³a
- **Bá»™ tá»‘i Æ°u**: Adam hoáº·c AdamW thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng nhá» kháº£ nÄƒng xá»­ lÃ½ gradient hiá»‡u quáº£.
- **Láº­p lá»‹ch há»c (Learning Rate Scheduling)**: Sá»­ dá»¥ng lá»‹ch há»c warmup (tÄƒng dáº§n learning rate á»Ÿ Ä‘áº§u) vÃ  decay (giáº£m dáº§n vá» cuá»‘i) Ä‘á»ƒ á»•n Ä‘á»‹nh huáº¥n luyá»‡n.
- **Batch Size**: CÃ¡c batch lá»›n (256-2048) thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº­n dá»¥ng tÃ­nh toÃ¡n song song.

## 5. á»¨ng dá»¥ng thá»±c tiá»…n cá»§a Transformer

Transformer Ä‘Ã£ Ä‘Æ°á»£c triá»ƒn khai trong nhiá»u bÃ i toÃ¡n NLP thÃ´ng qua thÆ° viá»‡n **ğŸ¤— Transformers** cá»§a Hugging Face. DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c vÃ­ dá»¥ chi tiáº¿t sá»­ dá»¥ng hÃ m `pipeline()`:

### 5.1. PhÃ¢n tÃ­ch cáº£m xÃºc (Sentiment Analysis)

### 5.2. Táº¡o vÄƒn báº£n (Text Generation)

### 5.3. Dá»‹ch mÃ¡y (Translation)

### 5.4. Nháº­n dáº¡ng thá»±c thá»ƒ (Named Entity Recognition - NER)

### 5.5. Äiá»n chá»— trá»‘ng (Fill-Mask)

### 5.6. Há»i Ä‘Ã¡p (Question Answering)

### 5.7. TÃ³m táº¯t vÄƒn báº£n (Summarization)
**Link tham kháº£o:** https://huggingface.co/learn/llm-course/vi/chapter1/3?fw=pt

## 6. CÃ¡c biáº¿n thá»ƒ cá»§a Transformer

Transformer Ä‘Ã£ truyá»n cáº£m há»©ng cho nhiá»u mÃ´ hÃ¬nh tiÃªn tiáº¿n, Ä‘Æ°á»£c chia thÃ nh ba nhÃ³m chÃ­nh:

### 6.1. MÃ´ hÃ¬nh chá»‰ dÃ¹ng Encoder (Auto-Encoding)
- **Äáº·c Ä‘iá»ƒm**:
  - Chá»‰ sá»­ dá»¥ng pháº§n Encoder cá»§a Transformer.
  - Self-Attention lÃ  "hai chiá»u" (bidirectional), cho phÃ©p chÃº Ã½ Ä‘áº¿n toÃ n bá»™ chuá»—i Ä‘áº§u vÃ o.
  - Huáº¥n luyá»‡n trÆ°á»›c báº±ng cÃ¡ch che ngáº«u nhiÃªn má»™t sá»‘ tá»« vÃ  yÃªu cáº§u mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n (Masked Language Modeling).
- **VÃ­ dá»¥**:
  - **BERT (Bidirectional Encoder Representations from Transformers)**: ÄÆ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ hiá»ƒu ngá»¯ cáº£nh hai chiá»u, phÃ¹ há»£p cho phÃ¢n loáº¡i, NER, há»i Ä‘Ã¡p.
  - **RoBERTa**: Cáº£i tiáº¿n BERT vá»›i dá»¯ liá»‡u huáº¥n luyá»‡n lá»›n hÆ¡n vÃ  tá»‘i Æ°u hÃ³a tá»‘t hÆ¡n.
  - **DistilBERT**: PhiÃªn báº£n nháº¹ cá»§a BERT, nhanh hÆ¡n 60% vÃ  giá»¯ 97% hiá»‡u suáº¥t.
- **á»¨ng dá»¥ng**: PhÃ¢n loáº¡i cÃ¢u, nháº­n dáº¡ng thá»±c thá»ƒ, tráº£ lá»i cÃ¢u há»i chiáº¿t xuáº¥t.

### 6.2. MÃ´ hÃ¬nh chá»‰ dÃ¹ng Decoder (Auto-Regressive)
- **Äáº·c Ä‘iá»ƒm**:
  - Chá»‰ sá»­ dá»¥ng pháº§n Decoder cá»§a Transformer.
  - Self-Attention lÃ  "má»™t chiá»u" (causal), chá»‰ chÃº Ã½ Ä‘áº¿n cÃ¡c tá»« trÆ°á»›c Ä‘Ã³ trong chuá»—i.
  - Huáº¥n luyá»‡n trÆ°á»›c báº±ng cÃ¡ch dá»± Ä‘oÃ¡n tá»« tiáº¿p theo (Causal Language Modeling).
- **VÃ­ dá»¥**:
  - **GPT (Generative Pre-trained Transformer)**: Tháº¿ há»‡ Ä‘áº§u tiÃªn, táº­p trung vÃ o táº¡o vÄƒn báº£n.
  - **GPT-2**: PhiÃªn báº£n lá»›n hÆ¡n, cÃ³ kháº£ nÄƒng táº¡o vÄƒn báº£n giá»‘ng con ngÆ°á»i.
  - **GPT-3**: MÃ´ hÃ¬nh cá»±c lá»›n vá»›i 175 tá»· tham sá»‘, há»— trá»£ zero-shot learning.
- **á»¨ng dá»¥ng**: Táº¡o vÄƒn báº£n, Ä‘á»‘i thoáº¡i, hoÃ n thÃ nh cÃ¢u.

### 6.3. MÃ´ hÃ¬nh Encoder-Decoder (Sequence-to-Sequence)
- **Äáº·c Ä‘iá»ƒm**:
  - Sá»­ dá»¥ng cáº£ Encoder vÃ  Decoder.
  - PhÃ¹ há»£p cho cÃ¡c bÃ i toÃ¡n cáº§n chuyá»ƒn Ä‘á»•i chuá»—i Ä‘áº§u vÃ o thÃ nh chuá»—i Ä‘áº§u ra.
  - Huáº¥n luyá»‡n trÆ°á»›c báº±ng cÃ¡c má»¥c tiÃªu phá»©c táº¡p, nhÆ° thay tháº¿ Ä‘oáº¡n vÄƒn báº£n báº±ng token Ä‘áº·c biá»‡t vÃ  dá»± Ä‘oÃ¡n Ä‘oáº¡n bá»‹ thay tháº¿.
- **VÃ­ dá»¥**:
  - **T5 (Text-to-Text Transfer Transformer)**: Xem má»i bÃ i toÃ¡n NLP nhÆ° má»™t bÃ i toÃ¡n text-to-text, vÃ­ dá»¥: dá»‹ch mÃ¡y, tÃ³m táº¯t, há»i Ä‘Ã¡p.
  - **BART**: Káº¿t há»£p huáº¥n luyá»‡n che dáº¥u vÃ  tÃ¡i táº¡o chuá»—i, tá»‘t cho tÃ³m táº¯t vÃ  dá»‹ch mÃ¡y.
  - **Marian**: Tá»‘i Æ°u cho dá»‹ch mÃ¡y vá»›i tá»‘c Ä‘á»™ cao.
- **á»¨ng dá»¥ng**: Dá»‹ch mÃ¡y, tÃ³m táº¯t, há»i Ä‘Ã¡p tá»•ng há»£p.

## 7. Tá»‘i Æ°u hÃ³a vÃ  má»Ÿ rá»™ng Transformer

Äá»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t vÃ  kháº£ nÄƒng má»Ÿ rá»™ng, Transformer Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a qua nhiá»u cÃ¡ch:

### 7.1. Ká»¹ thuáº­t tá»‘i Æ°u hÃ³a
- **Sparse Attention**: Giáº£m chi phÃ­ tÃ­nh toÃ¡n báº±ng cÃ¡ch chá»‰ tÃ­nh Attention cho má»™t táº­p há»£p con cÃ¡c tá»« thay vÃ¬ toÃ n bá»™ chuá»—i. VÃ­ dá»¥, Longformer sá»­ dá»¥ng Sparse Attention Ä‘á»ƒ xá»­ lÃ½ chuá»—i dÃ i hÃ ng nghÃ¬n tá»«.
- **Quantization**: Giáº£m Ä‘á»™ chÃ­nh xÃ¡c cá»§a trá»ng sá»‘ (tá»« float32 xuá»‘ng int8) Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ suy luáº­n vÃ  giáº£m bá»™ nhá»›.
- **Distillation**: Táº¡o cÃ¡c mÃ´ hÃ¬nh nhá» hÆ¡n (nhÆ° DistilBERT) tá»« cÃ¡c mÃ´ hÃ¬nh lá»›n, giá»¯ hiá»‡u suáº¥t cao nhÆ°ng nháº¹ hÆ¡n.

### 7.2. MÃ´ hÃ¬nh lá»›n hÆ¡n
- CÃ¡c mÃ´ hÃ¬nh nhÆ° GPT-3 (175 tá»· tham sá»‘) vÃ  PaLM (540 tá»· tham sá»‘) cho tháº¥y hiá»‡u suáº¥t tÄƒng theo kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh vÃ  dá»¯ liá»‡u huáº¥n luyá»‡n.
- Tuy nhiÃªn, mÃ´ hÃ¬nh lá»›n Ä‘Ã²i há»i chi phÃ­ tÃ­nh toÃ¡n khá»•ng lá»“ vÃ  cÃ³ tÃ¡c Ä‘á»™ng mÃ´i trÆ°á»ng Ä‘Ã¡ng ká»ƒ.

### 7.3. á»¨ng dá»¥ng ngoÃ i NLP
- **Vision Transformer (ViT)**: Ãp dá»¥ng Transformer cho hÃ¬nh áº£nh báº±ng cÃ¡ch chia áº£nh thÃ nh cÃ¡c "patch" vÃ  xá»­ lÃ½ chÃºng nhÆ° chuá»—i.
- **Audio Transformer**: Sá»­ dá»¥ng Transformer Ä‘á»ƒ xá»­ lÃ½ Ã¢m thanh, nhÆ° trong WaveNet hoáº·c Whisper (dá»‹ch giá»ng nÃ³i).
- **Reinforcement Learning**: Transformer Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ phÃ¢n tÃ­ch chiáº¿n lÆ°á»£c trong cÃ¡c trÃ² chÆ¡i nhÆ° AlphaStar.

## 8. Háº¡n cháº¿ vÃ  ThiÃªn kiáº¿n

Máº·c dÃ¹ máº¡nh máº½, Transformer cÃ³ má»™t sá»‘ háº¡n cháº¿ cáº§n lÆ°u Ã½:

### 8.1. ThiÃªn kiáº¿n
- **Nguá»“n dá»¯ liá»‡u**: Transformer thÆ°á»ng Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u internet lá»›n, chá»©a cáº£ ná»™i dung tÃ­ch cá»±c vÃ  tiÃªu cá»±c. Äiá»u nÃ y dáº«n Ä‘áº¿n thiÃªn kiáº¿n vá» giá»›i tÃ­nh, chá»§ng tá»™c, hoáº·c vÄƒn hÃ³a.
- **VÃ­ dá»¥**: Khi sá»­ dá»¥ng mÃ´ hÃ¬nh BERT cho tÃ¡c vá»¥ Ä‘iá»n chá»— trá»‘ng:
  ```python
  unmasker = pipeline("fill-mask", model="bert-base-uncased")
  print(unmasker("This man works as a [MASK].")[:2])
  print(unmasker("This woman works as a [MASK].")[:2])
  # Man: ['lawyer', 'doctor']
  # Woman: ['nurse', 'teacher']
  ```
  Káº¿t quáº£ cho tháº¥y mÃ´ hÃ¬nh liÃªn káº¿t "man" vá»›i cÃ¡c nghá» nhÆ° "lawyer", trong khi "woman" thÆ°á»ng gáº¯n vá»›i "nurse", pháº£n Ã¡nh thiÃªn kiáº¿n giá»›i tÃ­nh.

### 8.2. Chi phÃ­ tÃ­nh toÃ¡n
- Huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh lá»›n nhÆ° GPT-3 Ä‘Ã²i há»i hÃ ng triá»‡u USD vÃ  hÃ ng nghÃ¬n GPU, gÃ¢y ra lÆ°á»£ng khÃ­ tháº£i carbon Ä‘Ã¡ng ká»ƒ.
- Suy luáº­n (inference) trÃªn cÃ¡c mÃ´ hÃ¬nh lá»›n cÅ©ng tá»‘n kÃ©m, Ä‘áº·c biá»‡t trong cÃ¡c á»©ng dá»¥ng thá»i gian thá»±c.

### 8.3. Kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a
- MÃ´ hÃ¬nh cÃ³ thá»ƒ gáº·p khÃ³ khÄƒn vá»›i cÃ¡c ngÃ´n ngá»¯ hoáº·c ngá»¯ cáº£nh Ã­t Ä‘Æ°á»£c huáº¥n luyá»‡n, vÃ­ dá»¥: cÃ¡c ngÃ´n ngá»¯ thiá»ƒu sá»‘.
- Trong cÃ¡c bÃ i toÃ¡n phá»©c táº¡p, Transformer cÃ³ thá»ƒ táº¡o ra káº¿t quáº£ khÃ´ng chÃ­nh xÃ¡c hoáº·c "áº£o giÃ¡c" (hallucination), vÃ­ dá»¥: sinh vÄƒn báº£n sai sá»± tháº­t.

### 8.4. Giáº£i thÃ­ch káº¿t quáº£
- Transformer hoáº¡t Ä‘á»™ng nhÆ° má»™t "há»™p Ä‘en", khÃ³ giáº£i thÃ­ch táº¡i sao mÃ´ hÃ¬nh Ä‘Æ°a ra má»™t dá»± Ä‘oÃ¡n cá»¥ thá»ƒ. Äiá»u nÃ y gÃ¢y khÃ³ khÄƒn trong cÃ¡c á»©ng dá»¥ng yÃªu cáº§u Ä‘á»™ tin cáº­y cao, nhÆ° y táº¿ hoáº·c phÃ¡p luáº­t.

## 9. Káº¿t luáº­n

Transformer Ä‘Ã£ cÃ¡ch máº¡ng hÃ³a lÄ©nh vá»±c xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn vÃ  trÃ­ tuá»‡ nhÃ¢n táº¡o nhá» vÃ o cÆ¡ cháº¿ Attention vÃ  kháº£ nÄƒng xá»­ lÃ½ song song. Vá»›i kiáº¿n trÃºc linh hoáº¡t, nÃ³ lÃ  ná»n táº£ng cho cÃ¡c mÃ´ hÃ¬nh tiÃªn tiáº¿n nhÆ° BERT, GPT, T5, vÃ  Ä‘Æ°á»£c Ã¡p dá»¥ng trong nhiá»u lÄ©nh vá»±c tá»« dá»‹ch mÃ¡y Ä‘áº¿n xá»­ lÃ½ hÃ¬nh áº£nh. Tuy nhiÃªn, cÃ¡c háº¡n cháº¿ nhÆ° thiÃªn kiáº¿n, chi phÃ­ tÃ­nh toÃ¡n, vÃ  khÃ³ khÄƒn trong giáº£i thÃ­ch Ä‘Ã²i há»i sá»± cáº©n trá»ng khi triá»ƒn khai. Viá»‡c sá»­ dá»¥ng cÃ¡c cÃ´ng cá»¥ nhÆ° Hugging Face Transformers giÃºp dá»… dÃ ng Ã¡p dá»¥ng Transformer vÃ o thá»±c tiá»…n, má»Ÿ ra cÆ¡ há»™i cho cÃ¡c nhÃ  phÃ¡t triá»ƒn vÃ  nhÃ  nghiÃªn cá»©u.